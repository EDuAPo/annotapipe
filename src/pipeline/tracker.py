"""
è¿½è¸ªæ¨¡å—
è´Ÿè´£é£ä¹¦è¡¨æ ¼è¿½è¸ªå’Œæœ¬åœ°ç»Ÿè®¡
"""
import os
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime

import re

import yaml
import requests

logger = logging.getLogger(__name__)


def _extract_text_value(value) -> str:
    """ä»é£ä¹¦å­—æ®µå€¼ä¸­æå–çº¯æ–‡æœ¬ï¼ˆå¤„ç†å¤æ‚å¯¹è±¡æ ¼å¼ï¼‰"""
    if isinstance(value, str):
        return value
    if isinstance(value, list) and len(value) > 0:
        first = value[0]
        if isinstance(first, dict):
            return first.get('text', '')
        return str(first)
    return str(value) if value else ''


def extract_time_key(name: str) -> str:
    """
    ä»æ•°æ®åŒ…åç§°ä¸­æå–æ—¶é—´æ®µä½œä¸ºæ¨¡ç³ŠåŒ¹é…é”®
    
    æ”¯æŒçš„å‘½åæ ¼å¼:
    - 20251226_165741-165910_rere_0 -> 20251226_165741-165910 (å»æ‰åç¼€)
    - 20251124_132834_to_20251124_133029 -> 20251124_132834_to_20251124_133029 (ä¿æŒä¸å˜)
    - 1209_134548_134748 -> 1209_134548_134748 (ä¿æŒä¸å˜)
    """
    # æ ¼å¼: YYYYMMDD_HHMMSS-HHMMSS_xxx_n -> æå– YYYYMMDD_HHMMSS-HHMMSS
    match = re.match(r'^(\d{8}_\d{6}-\d{6})', name)
    if match:
        return match.group(1)
    
    # å…¶ä»–æ ¼å¼ä¿æŒä¸å˜
    return name


def _load_env_file(env_path: str = "configs/.env"):
    """æ‰‹åŠ¨åŠ è½½ .env æ–‡ä»¶åˆ°ç¯å¢ƒå˜é‡"""
    env_file = Path(env_path)
    if not env_file.exists():
        return
    try:
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#') or '=' not in line:
                    continue
                key, _, value = line.partition('=')
                key, value = key.strip(), value.strip()
                if key and key not in os.environ:
                    os.environ[key] = value
    except Exception:
        pass


@dataclass
class TrackingRecord:
    """è¿½è¸ªè®°å½•"""
    name: str
    keyframe_count: int = 0
    annotation_status: str = "å·²å®Œæˆ"
    uploaded: bool = False
    attributes: List[str] = None
    
    def __post_init__(self):
        if self.attributes is None:
            self.attributes = []


class BaseTracker:
    """è¿½è¸ªå™¨åŸºç±»"""
    
    def track(self, records: List[TrackingRecord]) -> Dict[str, Any]:
        """è¿½è¸ªè®°å½•"""
        raise NotImplementedError
    
    def detect_attributes(self, json_dir: str) -> List[str]:
        """æ£€æµ‹æ•°æ®å±æ€§"""
        return []


class LocalTracker(BaseTracker):
    """æœ¬åœ° TXT è¿½è¸ªå™¨"""
    
    def __init__(self, output_path: str = "local_report.txt"):
        self.output_path = Path(output_path)
    
    def track(self, records: List[TrackingRecord]) -> Dict[str, Any]:
        """å†™å…¥æœ¬åœ° TXT æŠ¥å‘Š"""
        with open(self.output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("æ ‡æ³¨æ•°æ®å¤„ç†ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            total_keyframes = 0
            for rec in records:
                f.write(f"æ•°æ®åŒ…: {rec.name}\n")
                f.write(f"  å…³é”®å¸§æ•°: {rec.keyframe_count}\n")
                f.write(f"  æ ‡æ³¨æƒ…å†µ: {rec.annotation_status}\n")
                f.write(f"  å·²ä¸Šä¼ : {'æ˜¯' if rec.uploaded else 'å¦'}\n")
                if rec.attributes:
                    f.write(f"  å±æ€§: {', '.join(rec.attributes)}\n")
                f.write("\n")
                total_keyframes += rec.keyframe_count
            
            f.write("-" * 60 + "\n")
            f.write(f"æ€»è®¡: {len(records)} ä¸ªæ•°æ®åŒ…, {total_keyframes} ä¸ªå…³é”®å¸§\n")
        
        logger.info(f"âœ… æœ¬åœ°æŠ¥å‘Šå·²ä¿å­˜: {self.output_path}")
        
        return {
            "created": len(records),
            "updated": 0,
            "total_keyframes": total_keyframes,
        }


class FeishuTracker(BaseTracker):
    """é£ä¹¦å¤šç»´è¡¨æ ¼è¿½è¸ªå™¨ï¼ˆç›´æ¥è°ƒç”¨é£ä¹¦ APIï¼‰"""
    
    def __init__(self, config_path: str = "configs/feishu.yaml"):
        self.config_path = Path(config_path)
        self.config: Dict = {}
        self._token: Optional[str] = None
        self._token_time: Optional[float] = None
        self._available = False
        self._records_cache: Optional[Dict[str, Dict]] = None  # ç¼“å­˜æ‰€æœ‰è®°å½•
        self._cache_time: Optional[float] = None
        self._init_config()
    
    def _init_config(self):
        """åŠ è½½é…ç½®"""
        try:
            # å…ˆåŠ è½½ .env æ–‡ä»¶
            _load_env_file()
            
            if not self.config_path.exists():
                logger.warning(f"é£ä¹¦é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
                return
            
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f) or {}
            
            # ä»ç¯å¢ƒå˜é‡è·å–æ•æ„Ÿä¿¡æ¯
            self.config['app_id'] = os.environ.get('FEISHU_APP_ID', '')
            self.config['app_secret'] = os.environ.get('FEISHU_APP_SECRET', '')
            
            if self.config.get('app_id') and self.config.get('app_secret'):
                self._available = True
                logger.info("ğŸ”— é£ä¹¦è¿½è¸ªå™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("é£ä¹¦å‡­è¯æœªé…ç½® (FEISHU_APP_ID/FEISHU_APP_SECRET)")
        except Exception as e:
            logger.warning(f"é£ä¹¦é…ç½®åŠ è½½å¤±è´¥: {e}")
    
    @property
    def is_available(self) -> bool:
        return self._available and self.config.get('enabled', True)
    
    def _get_token(self, force_refresh: bool = False) -> str:
        """è·å– tenant_access_token"""
        # å¦‚æœæœ‰ç¼“å­˜çš„tokenä¸”æœªè¿‡æœŸï¼Œç›´æ¥è¿”å›
        if not force_refresh and self._token and self._token_time:
            if time.time() - self._token_time < 6900:  # æå‰100ç§’åˆ·æ–°ï¼Œé¿å…è¾¹ç•Œæƒ…å†µ
                return self._token
        
        # è·å–æ–°token
        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        payload = {
            "app_id": self.config.get('app_id', ''),
            "app_secret": self.config.get('app_secret', '')
        }
        
        try:
            r = requests.post(url, json=payload, timeout=10)
            data = r.json()
            if data.get('code') == 0:
                self._token = data.get('tenant_access_token')
                self._token_time = time.time()
                logger.debug(f"âœ“ é£ä¹¦tokenè·å–æˆåŠŸ")
                return self._token
            else:
                logger.error(f"è·å–é£ä¹¦tokenå¤±è´¥: code={data.get('code')}, msg={data.get('msg')}")
                # æ¸…é™¤ç¼“å­˜çš„token
                self._token = None
                self._token_time = None
        except Exception as e:
            logger.error(f"è·å–é£ä¹¦tokenå¼‚å¸¸: {e}")
            # æ¸…é™¤ç¼“å­˜çš„token
            self._token = None
            self._token_time = None
        return ""
    
    def _get_headers(self, force_refresh: bool = False) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´ï¼Œå¦‚æœtokenæ— æ•ˆä¼šè‡ªåŠ¨åˆ·æ–°"""
        token = self._get_token(force_refresh=force_refresh)
        if not token:
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„é£ä¹¦token")
        return {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
    
    def _load_all_records(self, force_reload: bool = False) -> Dict[str, Dict]:
        """åŠ è½½æ‰€æœ‰è®°å½•åˆ°ç¼“å­˜ï¼Œè¿”å› {æ•°æ®åŒ…åç§°: {record_id, fields}}"""
        # ç¦ç”¨ç¼“å­˜ï¼Œæ¯æ¬¡éƒ½é‡æ–°åŠ è½½
        if not force_reload and self._records_cache is not None:
            return self._records_cache
        
        app_token = self.config.get('app_token', '')
        table_id = self.config.get('table_id', '')
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
        
        all_records = {}
        page_token = None
        page_count = 0
        
        logger.debug(f"ğŸ“¥ åŠ è½½é£ä¹¦è¡¨æ ¼æ‰€æœ‰è®°å½•... (app_token={app_token}, table_id={table_id})")
        
        while True:
            params = {"page_size": 500}
            if page_token:
                params["page_token"] = page_token
            
            try:
                # å°è¯•è·å–æ•°æ®ï¼Œå¦‚æœtokenå¤±æ•ˆåˆ™åˆ·æ–°åé‡è¯•
                for attempt in range(2):
                    try:
                        headers = self._get_headers(force_refresh=(attempt > 0))
                        r = requests.get(url, params=params, headers=headers, timeout=30)
                        data = r.json()
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯tokenå¤±æ•ˆé”™è¯¯
                        if data.get('code') == 99991663 and attempt == 0:
                            logger.warning("é£ä¹¦tokenå¤±æ•ˆï¼Œåˆ·æ–°åé‡è¯•...")
                            continue
                        
                        if data.get('code') != 0:
                            logger.error(f"åŠ è½½è®°å½•å¤±è´¥: code={data.get('code')}, msg={data.get('msg')}")
                            break
                        
                        # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        break
                    except Exception as e:
                        if attempt == 0:
                            logger.warning(f"è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­: {e}")
                            continue
                        raise
                
                items = data.get('data', {}).get('items', [])
                for item in items:
                    record_id = item.get('record_id')
                    fields = item.get('fields', {})
                    name = _extract_text_value(fields.get('æ•°æ®åŒ…åç§°', ''))
                    if name and record_id:
                        all_records[name] = {
                            'record_id': record_id,
                            'fields': fields
                        }
                
                page_count += 1
                page_token = data.get('data', {}).get('page_token')
                if not page_token or not data.get('data', {}).get('has_more'):
                    break
                    
            except Exception as e:
                logger.error(f"åŠ è½½è®°å½•å¼‚å¸¸: {e}")
                break
        
        logger.debug(f"ğŸ“¥ å·²åŠ è½½ {len(all_records)} æ¡è®°å½• ({page_count} é¡µ)")
        
        self._records_cache = all_records
        self._cache_time = time.time()
        return all_records
    
    def _search_record(self, name: str) -> Optional[Dict]:
        """æ ¹æ®æ•°æ®åŒ…åç§°æœç´¢è®°å½•ï¼Œè¿”å› {record_id, fields} æˆ– None
        
        åŒ¹é…ç­–ç•¥ï¼š
        1. å…ˆä»ç¼“å­˜ä¸­ç²¾ç¡®åŒ¹é…
        2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå†ä»ç¼“å­˜ä¸­æ¨¡ç³ŠåŒ¹é…ï¼ˆtime_keyï¼‰
        """
        # åŠ è½½æ‰€æœ‰è®°å½•åˆ°ç¼“å­˜
        all_records = self._load_all_records()
        
        # æå–æ—¶é—´æ®µä½œä¸ºåŒ¹é…é”®
        time_key = extract_time_key(name)
        
        # 1. ç²¾ç¡®åŒ¹é…
        if name in all_records:
            logger.debug(f"  âœ“ ç²¾ç¡®åŒ¹é…: {name}")
            return all_records[name]
        
        # 2. æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å« time_key çš„è®°å½•
        for existing_name, record in all_records.items():
            if time_key in existing_name or existing_name in name:
                logger.debug(f"  âœ“ æ¨¡ç³ŠåŒ¹é…: {name} -> {existing_name}")
                return record
        
        logger.debug(f"  âœ— æœªæ‰¾åˆ°: {name} (å°†æ–°å¢)")
        return None
    
    def _batch_create_records(self, records_fields: List[Dict]) -> tuple:
        """æ‰¹é‡åˆ›å»ºè®°å½•ï¼Œè¿”å› (åˆ›å»ºæ•°é‡, åˆ›å»ºçš„è®°å½•åˆ—è¡¨)"""
        if not records_fields:
            return 0, []
        
        app_token = self.config.get('app_token', '')
        table_id = self.config.get('table_id', '')
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/batch_create"
        
        payload = {"records": [{"fields": f} for f in records_fields]}
        
        # å°è¯•åˆ›å»ºï¼Œå¦‚æœtokenå¤±æ•ˆåˆ™åˆ·æ–°åé‡è¯•
        for attempt in range(2):
            try:
                headers = self._get_headers(force_refresh=(attempt > 0))
                r = requests.post(url, json=payload, headers=headers, timeout=30)
                data = r.json()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯tokenå¤±æ•ˆé”™è¯¯
                if data.get('code') == 99991663 and attempt == 0:
                    logger.warning("é£ä¹¦tokenå¤±æ•ˆï¼Œåˆ·æ–°åé‡è¯•...")
                    continue
                
                if data.get('code') == 0:
                    created_records = data.get('data', {}).get('records', [])
                    created = len(created_records)
                    # æ‰“å°åˆ›å»ºçš„è®°å½•è¯¦æƒ…
                    for rec in created_records:
                        rec_id = rec.get('record_id', 'N/A')
                        name = rec.get('fields', {}).get('æ•°æ®åŒ…åç§°', 'N/A')
                        logger.debug(f"  âœ“ å·²åˆ›å»º: {name} (record_id={rec_id})")
                    return created, created_records
                else:
                    logger.error(f"æ‰¹é‡åˆ›å»ºå¤±è´¥: code={data.get('code')}, msg={data.get('msg')}")
                    return 0, []
            except Exception as e:
                if attempt == 0:
                    logger.warning(f"åˆ›å»ºè¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­: {e}")
                    continue
                logger.error(f"æ‰¹é‡åˆ›å»ºå¼‚å¸¸: {e}")
                return 0, []
        
        return 0, []
    
    def _batch_update_records(self, records: List[Dict]) -> int:
        """æ‰¹é‡æ›´æ–°è®°å½•"""
        if not records:
            return 0
        
        app_token = self.config.get('app_token', '')
        table_id = self.config.get('table_id', '')
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/batch_update"
        
        # ç›´æ¥ä½¿ç”¨å­—æ®µåç§°
        payload = {"records": records}
        logger.debug(f"ğŸ“ æ›´æ–°è¯·æ±‚: record_id={records[0]['record_id'] if records else 'N/A'}, fields={records[0]['fields'] if records else {}}")
        
        # å°è¯•æ›´æ–°ï¼Œå¦‚æœtokenå¤±æ•ˆåˆ™åˆ·æ–°åé‡è¯•
        for attempt in range(2):
            try:
                headers = self._get_headers(force_refresh=(attempt > 0))
                r = requests.post(url, json=payload, headers=headers, timeout=30)
                data = r.json()
                logger.debug(f"ğŸ“ æ›´æ–°å“åº”: code={data.get('code')}, msg={data.get('msg', 'OK')}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯tokenå¤±æ•ˆé”™è¯¯
                if data.get('code') == 99991663 and attempt == 0:
                    logger.warning("é£ä¹¦tokenå¤±æ•ˆï¼Œåˆ·æ–°åé‡è¯•...")
                    continue
                
                if data.get('code') == 0:
                    updated_records = data.get('data', {}).get('records', [])
                    for rec in updated_records:
                        rec_id = rec.get('record_id', 'N/A')
                        name = rec.get('fields', {}).get('æ•°æ®åŒ…åç§°', 'N/A')
                        logger.debug(f"  âœ“ å·²æ›´æ–°: {name} (record_id={rec_id})")
                    return len(updated_records)
                else:
                    logger.error(f"æ‰¹é‡æ›´æ–°å¤±è´¥: code={data.get('code')}, msg={data.get('msg')}")
                    if records:
                        logger.error(f"æ›´æ–°è®°å½•ç¤ºä¾‹: {records[0]}")
                    return 0
            except Exception as e:
                if attempt == 0:
                    logger.warning(f"æ›´æ–°è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­: {e}")
                    continue
                logger.error(f"æ‰¹é‡æ›´æ–°å¼‚å¸¸: {e}")
                return 0
        
        return 0
    
    def _get_path_field_from_pipeline(self, pipeline_config_path: str) -> str:
        """ä» pipeline.yaml è¯»å– final_dir å¹¶è½¬æ¢ä¸ºé£ä¹¦åˆ—å
        
        ä¾‹å¦‚: /data02/dataset/scenesnew -> ä¸Šä¼ data02/dataset/scenesnew
        """
        try:
            config_path = Path(pipeline_config_path)
            if not config_path.exists():
                logger.warning(f"Pipelineé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {pipeline_config_path}ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
                return "ä¸Šä¼ data02/dataset/scenesnew"
            
            with open(config_path, 'r', encoding='utf-8') as f:
                pipeline_config = yaml.safe_load(f) or {}
            
            # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æœåŠ¡å™¨çš„ final_dir
            servers = pipeline_config.get('servers', [])
            for server in servers:
                if server.get('enabled', True):
                    final_dir = server.get('final_dir', '')
                    if final_dir:
                        # å»æ‰å¼€å¤´çš„ '/'ï¼Œç„¶ååŠ ä¸Š 'ä¸Šä¼ ' å‰ç¼€
                        path_without_slash = final_dir.lstrip('/')
                        path_field = f"ä¸Šä¼ {path_without_slash}"
                        logger.debug(f"ä» pipeline.yaml è¯»å–è·¯å¾„: {final_dir} -> {path_field}")
                        return path_field
            
            logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„æœåŠ¡å™¨é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
            return "ä¸Šä¼ data02/dataset/scenesnew"
        except Exception as e:
            logger.warning(f"è¯»å– pipeline.yaml å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
            return "ä¸Šä¼ data02/dataset/scenesnew"
    
    def detect_attributes(self, json_dir: str) -> List[str]:
        """ä»è·¯å¾„ä¸­æ£€æµ‹æ•°æ®å±æ€§"""
        attributes = []
        keywords = self.config.get('attribute_keywords', {})
        path_str = str(json_dir).lower()
        
        for attr_name, keywords_list in keywords.items():
            for keyword in keywords_list:
                if keyword.lower() in path_str:
                    attributes.append(attr_name)
                    break
        return attributes
    
    def track(self, records: List[TrackingRecord], json_dir: str = None, pipeline_config_path: str = "configs/pipeline.yaml") -> Dict[str, Any]:
        """è¿½è¸ªåˆ°é£ä¹¦è¡¨æ ¼"""
        if not self.is_available:
            logger.warning("é£ä¹¦è¿½è¸ªå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡")
            return {}
        
        # å¼ºåˆ¶é‡æ–°åŠ è½½è®°å½•ï¼Œç¡®ä¿ç¼“å­˜æ˜¯æœ€æ–°çš„
        self._load_all_records(force_reload=True)
        
        attributes = self.detect_attributes(json_dir) if json_dir else []
        
        # ä» pipeline.yaml åŠ¨æ€è¯»å– final_dir å¹¶è½¬æ¢ä¸ºé£ä¹¦åˆ—å
        path_field = self._get_path_field_from_pipeline(pipeline_config_path)
        field_mapping = self.config.get('field_mapping', {})
        
        to_create = []
        to_update = []
        created_names = []
        updated_names = []
        total_keyframes = 0
        
        logger.debug(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(records)} æ¡è®°å½•...")
        
        for rec in records:
            total_keyframes += rec.keyframe_count
            
            # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨
            logger.debug(f"ğŸ” æœç´¢è®°å½•: {rec.name}")
            existing = self._search_record(rec.name)
            
            # æ³¨æ„ï¼šé£ä¹¦å¤šç»´è¡¨æ ¼çš„å­—æ®µç±»å‹
            # åªæ›´æ–°å¤é€‰æ¡†ç±»å‹çš„å±æ€§/è·¯å¾„å­—æ®µ
            fields = {}
            
            if existing:
                # æ›´æ–°æ¨¡å¼ï¼šæ›´æ–°å…³é”®å¸§æ•°ã€æ ‡æ³¨æƒ…å†µã€æ›´æ–°æ—¶é—´å’Œå±æ€§/è·¯å¾„
                existing_fields = existing.get('fields', {})
                
                # æ›´æ–°å…³é”®å¸§æ•°ã€æ ‡æ³¨æƒ…å†µã€æ›´æ–°æ—¶é—´ï¼ˆæ³¨æ„å­—æ®µç±»å‹ï¼‰
                # å…³é”®å¸§æ•°æ˜¯æ–‡æœ¬ç±»å‹ï¼Œéœ€è¦å­—ç¬¦ä¸²
                # æ ‡æ³¨æƒ…å†µæ˜¯å¤šé€‰ç±»å‹ï¼Œéœ€è¦æ•°ç»„
                # æ›´æ–°æ—¶é—´æ˜¯æ—¥æœŸæ—¶é—´ç±»å‹ï¼Œéœ€è¦æ¯«ç§’æ—¶é—´æˆ³
                fields["å…³é”®å¸§æ•°"] = str(rec.keyframe_count)
                fields["æ ‡æ³¨æƒ…å†µ"] = [rec.annotation_status]
                fields["æ›´æ–°æ—¶é—´"] = int(time.time() * 1000)
                
                # å±æ€§åˆ—ï¼šä¿ç•™å·²æœ‰çš„ True å€¼ + æ–°å¢å½“å‰å±æ€§
                for attr_name in field_mapping.keys():
                    if attr_name.endswith('å±æ€§'):
                        if existing_fields.get(attr_name):
                            fields[attr_name] = True
                
                # æ–°å¢å½“å‰æ£€æµ‹åˆ°çš„å±æ€§
                for attr in attributes:
                    attr_field = f"{attr}å±æ€§"
                    if attr_field in field_mapping:
                        fields[attr_field] = True
                
                # è·¯å¾„åˆ—ï¼šä¿ç•™å·²æœ‰çš„ True å€¼ + æ–°å¢å½“å‰è·¯å¾„
                for field_name in field_mapping.keys():
                    if field_name.startswith('ä¸Šä¼ '):
                        if existing_fields.get(field_name):
                            fields[field_name] = True
                
                # æ–°å¢å½“å‰è·¯å¾„
                if rec.uploaded and path_field in field_mapping:
                    fields[path_field] = True
                
                to_update.append({"record_id": existing['record_id'], "fields": fields})
                updated_names.append(rec.name)
            else:
                # åˆ›å»ºæ¨¡å¼ï¼šè®¾ç½®åç§°ã€å…³é”®å¸§æ•°ã€æ ‡æ³¨æƒ…å†µã€æ›´æ–°æ—¶é—´å’Œå¤é€‰æ¡†å­—æ®µ
                # æ³¨æ„å­—æ®µç±»å‹ï¼šå…³é”®å¸§æ•°æ˜¯æ–‡æœ¬ï¼Œæ ‡æ³¨æƒ…å†µæ˜¯å¤šé€‰æ•°ç»„ï¼Œæ›´æ–°æ—¶é—´æ˜¯æ¯«ç§’æ—¶é—´æˆ³
                fields["æ•°æ®åŒ…åç§°"] = rec.name
                fields["å…³é”®å¸§æ•°"] = str(rec.keyframe_count)
                fields["æ ‡æ³¨æƒ…å†µ"] = [rec.annotation_status]
                fields["æ›´æ–°æ—¶é—´"] = int(time.time() * 1000)
                
                for attr in attributes:
                    attr_field = f"{attr}å±æ€§"
                    if attr_field in field_mapping:
                        fields[attr_field] = True
                
                if rec.uploaded and path_field in field_mapping:
                    fields[path_field] = True
                
                to_create.append(fields)
                created_names.append(rec.name)
        
        # æ‰§è¡Œæ‰¹é‡æ“ä½œï¼ˆé£ä¹¦é™åˆ¶æ¯æ‰¹ 500 æ¡ï¼‰
        created_count = 0
        updated_count = 0
        
        for i in range(0, len(to_create), 500):
            batch = to_create[i:i+500]
            count, created_records = self._batch_create_records(batch)
            created_count += count
            # æ›´æ–°ç¼“å­˜ï¼Œé¿å…åç»­é‡å¤åˆ›å»º
            for rec in created_records:
                name = rec.get('fields', {}).get('æ•°æ®åŒ…åç§°')
                if name and self._records_cache is not None:
                    self._records_cache[name] = {
                        'record_id': rec.get('record_id'),
                        'fields': rec.get('fields', {})
                    }
            if i + 500 < len(to_create):
                time.sleep(0.5)
        
        for i in range(0, len(to_update), 500):
            batch = to_update[i:i+500]
            updated_count += self._batch_update_records(batch)
            if i + 500 < len(to_update):
                time.sleep(0.5)
        
        logger.info(f"âœ… é£ä¹¦æ›´æ–°: æ–°å¢ {created_count}, æ›´æ–° {updated_count}")
        
        return {
            'created': created_names,
            'updated': updated_names,
            'total_keyframes': total_keyframes
        }


class Tracker:
    """ç»Ÿä¸€è¿½è¸ªå™¨ï¼Œè‡ªåŠ¨é€‰æ‹©é£ä¹¦æˆ–æœ¬åœ°"""
    
    def __init__(self, feishu_config: str = "configs/feishu.yaml"):
        self.feishu = FeishuTracker(feishu_config)
        self.local = LocalTracker()
        self._use_feishu = self.feishu.is_available
    
    def track(self, records: List[TrackingRecord], json_dir: str = None, pipeline_config_path: str = "configs/pipeline.yaml") -> Dict[str, Any]:
        """è¿½è¸ªè®°å½•"""
        if self._use_feishu:
            return self.feishu.track(records, json_dir, pipeline_config_path)
        else:
            return self.local.track(records)
    
    def detect_attributes(self, json_dir: str) -> List[str]:
        """æ£€æµ‹æ•°æ®å±æ€§"""
        if self._use_feishu:
            return self.feishu.detect_attributes(json_dir)
        return []


def create_tracking_records(result, keyframe_counts: Dict[str, int]) -> List[TrackingRecord]:
    """ä» PipelineResult åˆ›å»ºè¿½è¸ªè®°å½•"""
    records = []
    
    # æ”¶é›†æ‰€æœ‰å¤„ç†è¿‡çš„æ•°æ®åç§°
    all_names = set()
    all_names.update(result.downloaded)
    all_names.update(result.uploaded)
    all_names.update(result.processed)
    all_names.update(result.check_passed)
    all_names.update(result.check_failed)
    all_names.update(result.moved_to_final)
    all_names.update(result.skipped_server_exists)
    
    for name in sorted(all_names):
        # ç¡®å®šæ ‡æ³¨çŠ¶æ€
        if name in result.check_passed or name in result.skipped_server_exists:
            status = "å·²å®Œæˆ"
        elif name in result.check_failed:
            status = "æ£€æŸ¥ä¸é€šè¿‡"
        else:
            status = "å·²å®Œæˆ"
        
        # æ˜¯å¦å·²ä¸Šä¼ 
        uploaded = name in result.moved_to_final or name in result.skipped_server_exists
        
        records.append(TrackingRecord(
            name=name,
            keyframe_count=keyframe_counts.get(name, 0),
            annotation_status=status,
            uploaded=uploaded,
        ))
    
    return records
