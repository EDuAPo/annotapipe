"""
æµæ°´çº¿è¿è¡Œå™¨
è´Ÿè´£ç¼–æ’æ•´ä¸ªå¤„ç†æµç¨‹ï¼Œæ”¯æŒä¸²è¡Œ/å¹¶è¡Œæ¨¡å¼
"""
import sys
import time
import logging
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue

from .config import get_config, PipelineConfig
from .ssh_client import SSHClient
from .downloader import Downloader
from .processor import RemoteProcessor
from .server_logger import ServerLogger
from .tracker import Tracker, TrackingRecord
from .state import StateManager, ProcessStatus
from .nas_backup import NASBackup
from .utils import normalize_zip_name

logger = logging.getLogger(__name__)


@dataclass
class PipelineResult:
    """æµæ°´çº¿æ‰§è¡Œç»“æœ"""
    downloaded: List[str] = field(default_factory=list)
    download_failed: List[str] = field(default_factory=list)
    skipped_server_exists: List[str] = field(default_factory=list)
    uploaded: List[str] = field(default_factory=list)
    processed: List[str] = field(default_factory=list)
    check_passed: List[str] = field(default_factory=list)
    check_failed: List[str] = field(default_factory=list)
    moved_to_final: List[str] = field(default_factory=list)
    backed_up: List[str] = field(default_factory=list)  # NASå¤‡ä»½æˆåŠŸçš„æ•°æ®åŒ…
    backup_failed: List[str] = field(default_factory=list)  # NASå¤‡ä»½å¤±è´¥çš„æ•°æ®åŒ…
    keyframe_counts: Dict[str, int] = field(default_factory=dict)
    errors: Dict[str, List[tuple]] = field(default_factory=dict)
    
    def log_error(self, stem: str, step: str, msg: str):
        if stem not in self.errors:
            self.errors[stem] = []
        self.errors[stem].append((step, msg))


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""
    
    def __init__(self, total: int, title: str = "å¤„ç†è¿›åº¦"):
        self.total = total
        self.completed = 0
        self.success = 0
        self.failed = 0
        self.title = title
        self.lock = threading.Lock()
        self.start_time = datetime.now()
    
    def update(self, success: bool = True, name: str = ""):
        with self.lock:
            self.completed += 1
            if success:
                self.success += 1
            else:
                self.failed += 1
            self._display(name, success)
    
    def _display(self, name: str, success: bool):
        percent = self.completed / self.total * 100 if self.total > 0 else 0
        width = 25
        filled = int(width * self.completed / self.total) if self.total > 0 else 0
        bar = 'â”' * filled + 'â•¸' + 'â”€' * (width - filled - 1) if filled < width else 'â”' * width
        status = "âœ“" if success else "âœ—"
        
        sys.stdout.write(f'\r\033[K')
        sys.stdout.write(f'[{bar}] {self.completed}/{self.total} ({percent:.0f}%) â”‚ {status} {name[:30]:<30}')
        sys.stdout.flush()
        
        if self.completed >= self.total:
            print()
    
    def summary(self):
        elapsed = (datetime.now() - self.start_time).seconds
        mins, secs = divmod(elapsed, 60)
        print(f"\n{'â”€'*50}")
        print(f"  ğŸ“Š {self.title} å®Œæˆ")
        print(f"  âœ“ æˆåŠŸ: {self.success}  âœ— å¤±è´¥: {self.failed}  â± è€—æ—¶: {mins}åˆ†{secs}ç§’")
        print(f"{'â”€'*50}")


class SSHConnectionPool:
    """SSH è¿æ¥æ± ï¼Œç”¨äºå¹¶è¡Œæ¨¡å¼å¤ç”¨è¿æ¥"""
    
    def __init__(self, size: int = 3):
        self._pool: Queue = Queue()
        self._size = size
        self._created = 0
        self._lock = threading.Lock()
        self._scripts_deployed = False
    
    def get(self) -> Optional[SSHClient]:
        """è·å–ä¸€ä¸ªè¿æ¥"""
        # å…ˆå°è¯•ä»æ± ä¸­è·å–
        if not self._pool.empty():
            try:
                return self._pool.get_nowait()
            except:
                pass
        
        # åˆ›å»ºæ–°è¿æ¥
        with self._lock:
            if self._created < self._size:
                ssh = SSHClient()
                if ssh.connect():
                    self._created += 1
                    return ssh
        
        # ç­‰å¾…å¯ç”¨è¿æ¥
        return self._pool.get(timeout=60)
    
    def put(self, ssh: SSHClient):
        """å½’è¿˜è¿æ¥"""
        if ssh and ssh.is_connected:
            self._pool.put(ssh)
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        while not self._pool.empty():
            try:
                ssh = self._pool.get_nowait()
                ssh.close()
            except:
                pass


class PipelineRunner:
    """æµæ°´çº¿è¿è¡Œå™¨"""
    
    def __init__(self, json_dir: str, local_zip_dir: str = None, config: PipelineConfig = None):
        self.config = config or get_config()
        self.json_dir = Path(json_dir)
        
        # æœ¬åœ°ç›®å½•
        base_dir = Path(local_zip_dir) if local_zip_dir else Path(self.config.local_temp_dir)
        self.local_zip_dir = base_dir / "zips"
        self.local_check_dir = base_dir / "check_data"
        self.local_zip_dir.mkdir(parents=True, exist_ok=True)
        self.local_check_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»„ä»¶
        self.downloader = Downloader(self.config.dataweave)
        self.result = PipelineResult()
        self._lock = threading.Lock()
        self._deploy_lock = threading.Lock()
        self._scripts_deployed = False
        self.server_logger: Optional[ServerLogger] = None
        self.nas_backup: Optional[NASBackup] = None
        
        # çŠ¶æ€ç®¡ç†å™¨ï¼ˆæ–­ç‚¹ç»­ä¼ æ”¯æŒï¼‰
        self.state_manager = StateManager(base_dir)
    
    def run(self, mode: str = "optimized", workers: int = None):
        """
        è¿è¡Œæµæ°´çº¿
        mode: optimized (ä¸‹è½½å¹¶è¡Œ+æœåŠ¡å™¨ä¸²è¡Œ), parallel (å…¨å¹¶è¡Œ), streaming (æµå¼)
        """
        workers = workers or self.config.max_workers
        
        print()
        print("â•”" + "â•" * 50 + "â•—")
        print(f"â•‘  ğŸ“¦ æ ‡æ³¨æ•°æ®å¤„ç†æµæ°´çº¿ ({mode}æ¨¡å¼)".ljust(51) + "â•‘")
        print("â•š" + "â•" * 50 + "â•")
        print(f"  ğŸ“ JSONç›®å½•: {self.json_dir}")
        
        json_files = list(self.json_dir.glob("*.json"))
        if not json_files:
            print("  âš  æœªæ‰¾åˆ° JSON æ–‡ä»¶")
            return self.result
        
        print(f"  ğŸ“‹ å…± {len(json_files)} ä¸ªæ–‡ä»¶")
        
        # åˆå§‹åŒ–NASå¤‡ä»½ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
        with NASBackup() as nas_backup:
            self.nas_backup = nas_backup
            
            with SSHClient() as ssh:
                if not ssh.is_connected:
                    print("  âœ— æ— æ³•è¿æ¥æœåŠ¡å™¨")
                    return self.result
                
                print(f"  ğŸ”— å·²è¿æ¥æœåŠ¡å™¨: {ssh.server.ip}")
                
                processor = RemoteProcessor(ssh, self.config)
                processor.deploy_scripts()
                
                # åˆå§‹åŒ–æœåŠ¡å™¨æ—¥å¿—
                self.server_logger = ServerLogger(ssh)
                print(f"  ğŸ“‹ æœåŠ¡å™¨æ—¥å¿—: {self.server_logger.log_file}")
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                ssh.mkdir_p(ssh.server.zip_dir)
                ssh.mkdir_p(ssh.server.process_dir)
                
                # æ³¨æ„ï¼šä¸å†è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ 
                # å¦‚éœ€æ¸…ç†ï¼Œè¯·æ‰‹åŠ¨è°ƒç”¨ uploader.cleanup_incomplete(force=True)
                
                # è·å–æœåŠ¡å™¨çŠ¶æ€
                state = processor.get_server_state()
                print(f"  ğŸ“Š æœåŠ¡å™¨: {len(state['zip_files'])} ZIPs / {len(state['processed_dirs'])} å·²å®Œæˆ")
                
                # ç»Ÿè®¡æœ¬åœ°å·²ä¸‹è½½çš„æ–‡ä»¶ï¼ˆåªç»Ÿè®¡æ•°é‡ï¼Œä¸éªŒè¯å®Œæ•´æ€§ï¼‰
                local_zip_files = list(self.local_zip_dir.glob("*.zip"))
                print(f"  ğŸ’¾ æœ¬åœ°ZIP: {len(local_zip_files)} ä¸ª")
                
                # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œè·³è¿‡çš„æ–‡ä»¶ç«‹å³æ›´æ–°é£ä¹¦
                files_to_process = []
                tracker = Tracker()
                for json_file in json_files:
                    stem = json_file.stem
                    if stem in state['processed_dirs']:
                        # å°è¯•è·å–å…³é”®å¸§æ•°é‡ï¼Œå¦‚æœå¤±è´¥åˆ™é‡æ–°å¤„ç†
                        logger.info(f"[{stem}] æ£€æŸ¥final_dirä¸­çš„æ•°æ®å®Œæ•´æ€§...")
                        kf = processor.get_keyframe_count(f"{ssh.server.final_dir}/{stem}")
                        if kf > 0:
                            # æœåŠ¡å™¨ä¸Šå·²å®Œæˆä¸”æ•°æ®å®Œæ•´çš„æ–‡ä»¶ï¼Œè®°å½•ä¸ºè·³è¿‡
                            logger.info(f"[{stem}] âœ“ å·²åœ¨final_dirä¸­ (å…³é”®å¸§: {kf})ï¼Œè·³è¿‡æ‰€æœ‰æ­¥éª¤")
                            self.result.skipped_server_exists.append(stem)
                            self.result.check_passed.append(stem)
                            self.result.keyframe_counts[stem] = kf
                            # ç«‹å³æ›´æ–°é£ä¹¦
                            logger.info(f"[{stem}] æ›´æ–°é£ä¹¦è¡¨æ ¼...")
                            self._track_single_to_feishu(tracker, stem, silent=True)
                            logger.info(f"[{stem}] âœ“ é£ä¹¦å·²æ›´æ–°")
                        else:
                            # æ•°æ®ä¸å®Œæ•´ï¼Œéœ€è¦é‡æ–°å¤„ç†
                            logger.warning(f"[{stem}] âœ— åœ¨final_dirä½†æ•°æ®ä¸å®Œæ•´ï¼Œå°†é‡æ–°å¤„ç†")
                            files_to_process.append((json_file, stem))
                    else:
                        files_to_process.append((json_file, stem))
                
                skipped = len(json_files) - len(files_to_process)
                if skipped > 0:
                    print(f"  â­ è·³è¿‡(æœåŠ¡å™¨å·²å®Œæˆ): {skipped} ä¸ª")
                
                if not files_to_process:
                    print("  âœ“ æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ")
                    self._print_summary()
                    return self.result
                
                # è®¡ç®—å®é™…éœ€è¦ä¸‹è½½çš„æ•°é‡
                local_stems = set(f.stem for f in local_zip_files)
                need_download = 0
                for json_file, stem in files_to_process:
                    zip_name = f"{stem}.zip"
                    if zip_name not in state['zip_files'] and stem not in local_stems:
                        need_download += 1
                
                print(f"  ğŸ“¦ å¾…å¤„ç†: {len(files_to_process)} ä¸ª (éœ€ä¸‹è½½: {need_download})")
                if mode != "streaming":
                    print(f"  ğŸ§µ å¹¶å‘æ•°: {workers}")
                print()
                
                if mode == "optimized":
                    self._run_optimized(ssh, processor, files_to_process, state, workers)
                elif mode == "parallel":
                    self._run_parallel(processor, files_to_process, state, workers)
                else:
                    self._run_streaming(ssh, processor, files_to_process, state)
        
        self._print_summary()
        
        # æ³¨æ„ï¼šé£ä¹¦åŒæ­¥å·²åœ¨ _track_single_to_feishu ä¸­é€ä¸ªå®Œæˆ
        # ä¸å†è°ƒç”¨ _track_to_feishu é¿å…é‡å¤åŒæ­¥
        
        return self.result
    
    def _run_optimized(self, ssh: SSHClient, processor: RemoteProcessor,
                       files: List[tuple], state: Dict, workers: int):
        """ä¼˜åŒ–æ¨¡å¼ï¼šä¸‹è½½å¹¶è¡Œ + æœåŠ¡å™¨æ“ä½œä¸²è¡Œ"""
        
        # é˜¶æ®µ1: å¹¶è¡Œä¸‹è½½
        print("=" * 50)
        print("  ğŸ“¥ é˜¶æ®µ1: å¹¶è¡Œä¸‹è½½ ZIP æ–‡ä»¶")
        print("=" * 50)
        
        files_to_download = []
        skipped_local = 0
        skipped_server = 0
        for json_file, stem in files:
            # è§„èŒƒåŒ–æ–‡ä»¶åç”¨äºæŸ¥æ‰¾ZIP
            normalized_stem = normalize_zip_name(stem)
            zip_name = f"{normalized_stem}.zip"
            local_zip = self.local_zip_dir / zip_name
            
            if zip_name in state['zip_files']:
                self.result.skipped_server_exists.append(stem)
                skipped_server += 1
                continue
            # åªæ£€æŸ¥æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°>0ï¼Œä¸éªŒè¯å®Œæ•´æ€§ï¼ˆé¿å…å¡é¡¿ï¼‰
            if local_zip.exists() and local_zip.stat().st_size > 0:
                self.result.downloaded.append(stem)
                skipped_local += 1
                continue
            files_to_download.append((stem, zip_name, local_zip))
        
        if skipped_server > 0 or skipped_local > 0:
            print(f"  è·³è¿‡: æœåŠ¡å™¨å·²æœ‰ {skipped_server} ä¸ª, æœ¬åœ°å·²æœ‰ {skipped_local} ä¸ª")
        
        if files_to_download:
            print(f"  éœ€ä¸‹è½½: {len(files_to_download)} ä¸ªæ–‡ä»¶ (å¹¶å‘: {workers})")
            
            # é¢„å…ˆè·å– tokenï¼Œé¿å…åœ¨è¿›åº¦æ¡æ˜¾ç¤ºæœŸé—´è¾“å‡ºæ—¥å¿—
            self.downloader.token_manager.get_token()
            
            # å°è¯•ä½¿ç”¨ tqdm è¿›åº¦æ¡
            try:
                from tqdm import tqdm
                use_tqdm = True
            except ImportError:
                use_tqdm = False
            
            # å¹¶è¡Œä¸‹è½½
            download_status = {}  # stem -> status
            status_lock = threading.Lock()
            active_downloads = {}  # stem -> (downloaded, total)
            
            if use_tqdm:
                # å•è¿›åº¦æ¡ï¼šæ˜¾ç¤ºæ–‡ä»¶æ•° + ä¸‹è½½æµé‡
                file_pbar = tqdm(total=len(files_to_download), desc="  ä¸‹è½½è¿›åº¦", 
                                unit="ä¸ª", ncols=80, leave=True)
                total_bytes = [0]
                downloaded_bytes = [0]
                last_update = [time.time()]
                start_time = [time.time()]
                
                def make_progress_callback(stem):
                    def callback(downloaded, total):
                        with status_lock:
                            if stem not in active_downloads:
                                active_downloads[stem] = (0, total)
                                if total > 0:
                                    total_bytes[0] += total
                            old_downloaded, _ = active_downloads[stem]
                            delta = downloaded - old_downloaded
                            if delta > 0:
                                active_downloads[stem] = (downloaded, total)
                                downloaded_bytes[0] += delta
                                # é™åˆ¶åˆ·æ–°é¢‘ç‡ï¼Œé¿å…é—ªçƒ
                                now = time.time()
                                if now - last_update[0] > 0.2:
                                    last_update[0] = now
                                    # è®¡ç®—ä¸‹è½½é€Ÿåº¦
                                    elapsed = now - start_time[0]
                                    speed = downloaded_bytes[0] / elapsed / 1024 / 1024 if elapsed > 0 else 0
                                    file_pbar.set_postfix_str(f"{downloaded_bytes[0]/1024/1024:.0f}MB {speed:.1f}MB/s", refresh=True)
                    return callback
            else:
                make_progress_callback = lambda stem: None
            
            def download_task(stem, zip_name, local_zip):
                try:
                    progress_cb = make_progress_callback(stem)
                    success = self.downloader.download_file(zip_name, local_zip, progress_callback=progress_cb)
                    with status_lock:
                        download_status[stem] = success
                    return stem, success
                except Exception as e:
                    with status_lock:
                        download_status[stem] = False
                    logger.error(f"ä¸‹è½½å¼‚å¸¸ {stem}: {e}")
                    return stem, False
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                futures = [executor.submit(download_task, stem, zip_name, local_zip) 
                          for stem, zip_name, local_zip in files_to_download]
                
                for future in as_completed(futures):
                    stem, success = future.result()
                    if use_tqdm:
                        file_pbar.update(1)
                        status = "âœ“" if success else "âœ—"
                        file_pbar.set_postfix_str(f"{status} {stem[:20]}")
                    else:
                        status = "âœ“" if success else "âœ—"
                        sys.stdout.write(f'\r\033[K  [{len(download_status)}/{len(futures)}] {status} {stem[:40]}')
                        sys.stdout.flush()
                    
                    if success:
                        self.result.downloaded.append(stem)
                    else:
                        self.result.download_failed.append(stem)
            
            if use_tqdm:
                file_pbar.close()
            else:
                print()
            
            # ä¸‹è½½æ±‡æ€»
            success_count = len(self.result.downloaded) - skipped_local
            fail_count = len(self.result.download_failed)
            print(f"  ğŸ“Š ä¸‹è½½å®Œæˆ: âœ“ {success_count}  âœ— {fail_count}")
        else:
            print("  æ‰€æœ‰æ–‡ä»¶å·²ä¸‹è½½æˆ–æœåŠ¡å™¨å·²å­˜åœ¨")
        
        # é˜¶æ®µ2: ä¸²è¡ŒæœåŠ¡å™¨æ“ä½œ
        print()
        print("=" * 50)
        print("  ğŸ”„ é˜¶æ®µ2: ä¸²è¡ŒæœåŠ¡å™¨æ“ä½œ")
        print("=" * 50)
        
        files_for_server = [(jf, stem) for jf, stem in files if stem not in self.result.download_failed]
        
        if not files_for_server:
            print("  æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
            return
        
        # è®¡ç®—å®é™…éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶æ•°é‡ï¼ˆæ’é™¤æœåŠ¡å™¨å·²æœ‰ZIPçš„ï¼‰
        need_upload_count = sum(1 for jf, stem in files_for_server 
                                if f"{stem}.zip" not in state['zip_files'] 
                                and (self.local_zip_dir / f"{stem}.zip").exists())
        
        print(f"  å¾…å¤„ç†: {len(files_for_server)} ä¸ª (éœ€ä¸Šä¼ : {need_upload_count} ä¸ª)")
        
        progress = ProgressTracker(len(files_for_server), "æœåŠ¡å™¨å¤„ç†")
        tracker = Tracker()
        upload_idx = 0
        
        for idx, (json_file, stem) in enumerate(files_for_server, 1):
            zip_name = f"{stem}.zip"
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸Šä¼ 
            need_upload = zip_name not in state['zip_files'] and (self.local_zip_dir / zip_name).exists()
            if need_upload:
                upload_idx += 1
                success = self._process_single(ssh, processor, json_file, stem, state, upload_idx, need_upload_count)
            else:
                success = self._process_single(ssh, processor, json_file, stem, state, 0, 0)
            # åªåœ¨æˆåŠŸæ—¶åŒæ­¥é£ä¹¦
            if success:
                self._track_single_to_feishu(tracker, stem, silent=True)
            progress.update(success=success, name=stem)
        
        progress.summary()
    
    def _run_parallel(self, processor: RemoteProcessor, files: List[tuple], 
                      state: Dict, workers: int):
        """å…¨å¹¶è¡Œæ¨¡å¼ï¼šä½¿ç”¨è¿æ¥æ± å¤ç”¨ SSH è¿æ¥"""
        progress = ProgressTracker(len(files), "å¹¶è¡Œå¤„ç†")
        pool = SSHConnectionPool(size=workers)
        tracker = Tracker()
        
        try:
            with ThreadPoolExecutor(max_workers=workers) as executor:
                futures = {}
                for json_file, stem in files:
                    future = executor.submit(
                        self._process_with_pool, 
                        pool, json_file, stem, state
                    )
                    futures[future] = stem
                
                for future in as_completed(futures):
                    stem = futures[future]
                    try:
                        success = future.result()
                        # åªåœ¨æˆåŠŸæ—¶åŒæ­¥é£ä¹¦
                        if success:
                            self._track_single_to_feishu(tracker, stem, silent=True)
                        progress.update(success=success, name=stem)
                    except Exception as e:
                        logger.error(f"å¹¶è¡Œå¤„ç†å¼‚å¸¸ {stem}: {e}")
                        progress.update(success=False, name=f"{stem} (å¼‚å¸¸)")
        finally:
            pool.close_all()
        
        progress.summary()
    
    def _run_streaming(self, ssh: SSHClient, processor: RemoteProcessor,
                       files: List[tuple], state: Dict):
        """æµå¼æ¨¡å¼ï¼šä¸‹è½½ä¸€ä¸ªå¤„ç†ä¸€ä¸ªï¼Œæ¯å®Œæˆä¸€ä¸ªç«‹å³åŒæ­¥é£ä¹¦"""
        progress = ProgressTracker(len(files), "æµå¼å¤„ç†")
        tracker = Tracker()
        
        # é¢„è®¡ç®—éœ€è¦ä¸‹è½½å’Œä¸Šä¼ çš„æ–‡ä»¶
        local_stems = set(f.stem for f in self.local_zip_dir.glob("*.zip"))
        need_download_list = []
        need_upload_list = []
        
        for json_file, stem in files:
            # è§„èŒƒåŒ–æ–‡ä»¶åç”¨äºæŸ¥æ‰¾ZIP
            normalized_stem = normalize_zip_name(stem)
            zip_name = f"{normalized_stem}.zip"
            if zip_name not in state['zip_files'] and stem not in local_stems:
                need_download_list.append(stem)
            if zip_name not in state['zip_files']:
                need_upload_list.append(stem)
        
        download_idx = 0
        upload_idx = 0
        need_download_count = len(need_download_list)
        need_upload_count = len(need_upload_list)
        
        for json_file, stem in files:
            zip_name = f"{stem}.zip"
            
            # è®¡ç®—å½“å‰æ–‡ä»¶çš„è¿›åº¦ç´¢å¼•
            need_download = stem in need_download_list
            need_upload = stem in need_upload_list
            
            if need_download:
                download_idx += 1
                current_idx = download_idx
                total_count = need_download_count
            elif need_upload:
                upload_idx += 1
                current_idx = upload_idx
                total_count = need_upload_count
            else:
                current_idx = 0
                total_count = 0
            
            success = self._process_single(ssh, processor, json_file, stem, state, current_idx, total_count)
            # åªåœ¨æˆåŠŸæ—¶åŒæ­¥é£ä¹¦
            if success:
                self._track_single_to_feishu(tracker, stem, silent=True)
            progress.update(success=success, name=stem)
        
        progress.summary()
    
    def _process_single(self, ssh: SSHClient, processor: RemoteProcessor,
                        json_file: Path, stem: str, state: Dict,
                        current_idx: int = 0, total_count: int = 0) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆä½¿ç”¨å…±äº«SSHè¿æ¥ï¼‰"""
        # è§„èŒƒåŒ–æ–‡ä»¶åç”¨äºæŸ¥æ‰¾ZIP
        normalized_stem = normalize_zip_name(stem)
        zip_name = f"{normalized_stem}.zip"
        local_zip = self.local_zip_dir / zip_name
        server = ssh.server
        
        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰ ZIP æ–‡ä»¶ï¼ˆå¯èƒ½å¸¦æœ‰ processed_ å‰ç¼€ï¼‰
        server_has_zip = zip_name in state['zip_files']
        actual_zip_name = state.get('zip_file_map', {}).get(zip_name, zip_name)
        remote_zip = f"{server.zip_dir}/{actual_zip_name}"
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»ä¸­é—´çŠ¶æ€æ¢å¤
        skip_download = self.state_manager.can_skip_download(stem)
        skip_upload = self.state_manager.can_skip_upload(stem)
        
        # æ£€æŸ¥ process_dir ä¸­æ˜¯å¦å·²æœ‰è§£å‹çš„æ•°æ®
        in_processing = stem in state.get('processing_dirs', set())
        
        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä¸éªŒè¯å®Œæ•´æ€§ï¼Œé¿å…å¡é¡¿ï¼‰
        local_exists = local_zip.exists() and local_zip.stat().st_size > 0
        
        try:
            # æ­¥éª¤1: ä¸‹è½½ZIPï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not skip_download and not server_has_zip and not local_exists:
                logger.info(f"[{stem}] â¬‡ ä¸‹è½½ZIP...")
                download_start = [time.time()]
                last_print = [0]
                
                def download_progress(downloaded, total):
                    now = time.time()
                    if now - last_print[0] < 0.3:
                        return
                    last_print[0] = now
                    elapsed = now - download_start[0]
                    speed = downloaded / elapsed / 1024 / 1024 if elapsed > 0 else 0
                    percent = downloaded / total * 100 if total > 0 else 0
                    sys.stdout.write(f'\r\033[K  â¬‡ ä¸‹è½½ {stem[:20]}: {downloaded/1024/1024:.1f}/{total/1024/1024:.1f}MB ({percent:.0f}%) {speed:.1f}MB/s')
                    sys.stdout.flush()
                
                if not self.downloader.download_file(zip_name, local_zip, progress_callback=download_progress):
                    print()
                    logger.error(f"[{stem}] ä¸‹è½½å¤±è´¥")
                    self.result.log_error(stem, "ä¸‹è½½", "ä¸‹è½½å¤±è´¥")
                    self.result.check_failed.append(stem)
                    self.state_manager.update(stem, ProcessStatus.FAILED, "ä¸‹è½½å¤±è´¥")
                    return False
                print()
                self.result.downloaded.append(stem)
                self.state_manager.update(stem, ProcessStatus.DOWNLOADED)
            else:
                if server_has_zip:
                    logger.info(f"[{stem}] â­ è·³è¿‡ä¸‹è½½ (æœåŠ¡å™¨å·²æœ‰)")
                elif local_exists:
                    logger.info(f"[{stem}] â­ è·³è¿‡ä¸‹è½½ (æœ¬åœ°å·²æœ‰)")
                elif skip_download:
                    logger.info(f"[{stem}] â­ è·³è¿‡ä¸‹è½½ (æ–­ç‚¹ç»­ä¼ )")
            
            # æ­¥éª¤2: ä¸Šä¼ ZIPåˆ°æœåŠ¡å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not skip_upload and not server_has_zip and local_zip.exists():
                logger.info(f"[{stem}] â¬† ä¸Šä¼ ZIP...")
                upload_start = [time.time()]
                last_print = [0]
                
                def upload_progress(transferred, total):
                    now = time.time()
                    if now - last_print[0] < 0.3:
                        return
                    last_print[0] = now
                    elapsed = now - upload_start[0]
                    speed = transferred / elapsed / 1024 / 1024 if elapsed > 0 else 0
                    percent = transferred / total * 100 if total > 0 else 0
                    sys.stdout.write(f'\r\033[K  â¬† ä¸Šä¼  {stem[:20]}: {transferred/1024/1024:.1f}/{total/1024/1024:.1f}MB ({percent:.0f}%) {speed:.1f}MB/s')
                    sys.stdout.flush()
                
                if not ssh.upload_file(str(local_zip), remote_zip, progress_callback=upload_progress):
                    print()
                    logger.error(f"[{stem}] ä¸Šä¼ å¤±è´¥")
                    self.result.log_error(stem, "ä¸Šä¼ ", "ä¸Šä¼ å¤±è´¥")
                    self.result.check_failed.append(stem)
                    self.state_manager.update(stem, ProcessStatus.FAILED, "ä¸Šä¼ å¤±è´¥")
                    return False
                print()
                self.result.uploaded.append(stem)
                self.state_manager.update(stem, ProcessStatus.UPLOADED)
            else:
                if server_has_zip:
                    logger.info(f"[{stem}] â­ è·³è¿‡ä¸Šä¼  (æœåŠ¡å™¨å·²æœ‰)")
                elif skip_upload:
                    logger.info(f"[{stem}] â­ è·³è¿‡ä¸Šä¼  (æ–­ç‚¹ç»­ä¼ )")
            
            # æ­¥éª¤3-5: è§£å‹ZIPå¹¶ä¸Šä¼ JSON
            data_dir = f"{server.process_dir}/{stem}"
            need_extract = True
            
            if in_processing:
                # ç›®å½•å·²å­˜åœ¨ï¼ŒéªŒè¯æ•°æ®å®Œæ•´æ€§
                logger.info(f"[{stem}] ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
                kf_check = processor.get_keyframe_count(data_dir)
                if kf_check > 0:
                    logger.info(f"[{stem}] âœ“ æ•°æ®å®Œæ•´ï¼Œè·³è¿‡è§£å‹")
                    need_extract = False
                else:
                    logger.warning(f"[{stem}] âœ— æ•°æ®ä¸å®Œæ•´ï¼Œé‡æ–°è§£å‹")
                    ssh.exec_command(f"rm -rf '{data_dir}'")
            
            if need_extract:
                # å°è¯•è§£å‹ï¼Œå¤±è´¥æ—¶æ¸…ç†å¹¶é‡è¯•ä¸€æ¬¡
                max_retries = 2
                for attempt in range(max_retries):
                    success, err = processor.process_zip(remote_zip, str(json_file), stem)
                    if success:
                        break
                    
                    # è§£å‹å¤±è´¥
                    if attempt < max_retries - 1:
                        # è¿˜æœ‰é‡è¯•æœºä¼šï¼Œæ¸…ç†å¹¶é‡è¯•
                        logger.warning(f"[{stem}] è§£å‹å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {err[:100]}")
                        logger.info(f"[{stem}] æ¸…ç†ä¸å®Œæ•´æ•°æ®å¹¶é‡è¯•...")
                        ssh.exec_command(f"rm -rf '{data_dir}'")
                    else:
                        # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥äº†
                        logger.error(f"[{stem}] è§£å‹å¤±è´¥ ({max_retries}æ¬¡å°è¯•): {err}")
                        logger.info(f"[{stem}] æ¸…ç†ä¸å®Œæ•´çš„æ•°æ®...")
                        ssh.exec_command(f"rm -rf '{data_dir}'")
                        self.result.log_error(stem, "å¤„ç†", err)
                        self.result.check_failed.append(stem)
                        self.state_manager.update(stem, ProcessStatus.FAILED, err)
                        return False
            
            self.result.processed.append(stem)
            self.state_manager.update(stem, ProcessStatus.PROCESSED)
            
            # æ£€æŸ¥æ ‡æ³¨è´¨é‡
            logger.info(f"[{stem}] ğŸ” æ£€æŸ¥è´¨é‡...")
            passed, issue_count, report = processor.check_annotations(data_dir, stem)
            
            # è·å–å…³é”®å¸§æ•°é‡
            kf = processor.get_keyframe_count(data_dir)
            self.result.keyframe_counts[stem] = kf
            
            if not passed:
                logger.error(f"[{stem}] æ£€æŸ¥æœªé€šè¿‡: å‘ç° {issue_count} ä¸ªé—®é¢˜å¸§")
                self.result.log_error(stem, "æ£€æŸ¥", f"å‘ç° {issue_count} ä¸ªé—®é¢˜å¸§")
                self.result.check_failed.append(stem)
                self.state_manager.update(stem, ProcessStatus.CHECKED, f"æ£€æŸ¥å¤±è´¥: {issue_count} ä¸ªé—®é¢˜å¸§")
                local_report = self.local_check_dir / f"report_{stem}.txt"
                ssh.download_file(report, str(local_report))
                return False
            
            logger.info(f"[{stem}] âœ“ è´¨é‡æ£€æŸ¥é€šè¿‡ ({kf}å¸§)")
            self.result.check_passed.append(stem)
            self.state_manager.update(stem, ProcessStatus.CHECKED)
            
            # ç§»åŠ¨åˆ°final_dir
            logger.info(f"[{stem}] ğŸ“ ç§»åŠ¨åˆ°final_dir...")
            success, dst = processor.move_to_final(stem)
            if success:
                logger.info(f"[{stem}] âœ“ å®Œæˆ")
                self.result.moved_to_final.append(stem)
                if self.server_logger:
                    self.server_logger.log_success(stem, kf)
                self.state_manager.update(stem, ProcessStatus.COMPLETED)
                
                # å¤‡ä»½åˆ°NASï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.nas_backup and self.nas_backup.is_enabled:
                    print(f"  ğŸ’¾ å¤‡ä»½åˆ°NAS: {stem}")
                    logger.info(f"[{stem}] ğŸ’¾ å¤‡ä»½åˆ°NAS...")
                    backup_success, backup_msg = self.nas_backup.backup_data(
                        source_dir=dst,
                        final_dir=server.final_dir,
                        data_name=stem
                    )
                    if backup_success:
                        print(f"  âœ“ NASå¤‡ä»½æˆåŠŸ: {stem}")
                        logger.info(f"[{stem}] âœ“ NASå¤‡ä»½æˆåŠŸ")
                        self.result.backed_up.append(stem)
                    else:
                        print(f"  âœ— NASå¤‡ä»½å¤±è´¥: {stem} - {backup_msg}")
                        logger.warning(f"[{stem}] âœ— NASå¤‡ä»½å¤±è´¥: {backup_msg}")
                        self.result.backup_failed.append(stem)
                        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç»§ç»­
                        backup_config = self.nas_backup.config.get('backup', {})
                        if backup_config.get('on_error', 'continue') == 'stop':
                            self.result.log_error(stem, "NASå¤‡ä»½", backup_msg)
                            return False
            else:
                logger.error(f"[{stem}] âœ— ç§»åŠ¨å¤±è´¥: {dst}")
                self.result.log_error(stem, "ç§»åŠ¨", dst)
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"[{stem}] å¼‚å¸¸: {e}")
            self.result.log_error(stem, "å¼‚å¸¸", str(e))
            self.result.check_failed.append(stem)
            self.state_manager.update(stem, ProcessStatus.FAILED, str(e))
            if self.server_logger:
                self.server_logger.log_failure(stem, str(e))
            return False
    
    def _process_with_pool(self, pool: SSHConnectionPool, json_file: Path, 
                           stem: str, state: Dict) -> bool:
        """ä½¿ç”¨è¿æ¥æ± å¤„ç†å•ä¸ªæ–‡ä»¶"""
        ssh = None
        try:
            ssh = pool.get()
            if not ssh or not ssh.is_connected:
                self.result.log_error(stem, "è¿æ¥", "æ— æ³•è·å–SSHè¿æ¥")
                with self._lock:
                    self.result.check_failed.append(stem)
                return False
            
            processor = RemoteProcessor(ssh, self.config)
            
            # çº¿ç¨‹å®‰å…¨çš„è„šæœ¬éƒ¨ç½²ï¼ˆåªéƒ¨ç½²ä¸€æ¬¡ï¼‰
            with self._deploy_lock:
                if not self._scripts_deployed:
                    processor.deploy_scripts()
                    self._scripts_deployed = True
            
            return self._process_single(ssh, processor, json_file, stem, state)
        finally:
            if ssh:
                pool.put(ssh)
    
    def _process_single_threaded(self, json_file: Path, stem: str, state: Dict) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç‹¬ç«‹SSHè¿æ¥ï¼Œç”¨äºå¹¶è¡Œæ¨¡å¼ï¼‰- å·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹"""
        with SSHClient() as ssh:
            if not ssh.is_connected:
                self.result.log_error(stem, "è¿æ¥", "SSHè¿æ¥å¤±è´¥")
                with self._lock:
                    self.result.check_failed.append(stem)
                return False
            
            processor = RemoteProcessor(ssh, self.config)
            processor.deploy_scripts()
            return self._process_single(ssh, processor, json_file, stem, state)
    
    def _print_summary(self):
        """æ‰“å°æ‰§è¡Œæ±‡æ€»"""
        print()
        print("â•”" + "â•" * 50 + "â•—")
        print("â•‘  ğŸ“Š æ‰§è¡Œæ±‡æ€»".ljust(51) + "â•‘")
        print("â• " + "â•" * 50 + "â•£")
        
        stats = [
            ("â­ è·³è¿‡(å·²å­˜åœ¨)", len(self.result.skipped_server_exists)),
            ("â¬‡ ä¸‹è½½æˆåŠŸ", len(self.result.downloaded)),
            ("â¬‡ ä¸‹è½½å¤±è´¥", len(self.result.download_failed)),
            ("â¬† ä¸Šä¼ æˆåŠŸ", len(self.result.uploaded)),
            ("âš™ å¤„ç†æˆåŠŸ", len(self.result.processed)),
            ("âœ“ æ£€æŸ¥é€šè¿‡", len(self.result.check_passed)),
            ("âœ— æ£€æŸ¥å¤±è´¥", len(self.result.check_failed)),
            ("ğŸ“ å·²ç§»åŠ¨", len(self.result.moved_to_final)),
            ("ğŸ’¾ NASå¤‡ä»½æˆåŠŸ", len(self.result.backed_up)),
            ("ğŸ’¾ NASå¤‡ä»½å¤±è´¥", len(self.result.backup_failed)),
        ]
        
        total_kf = sum(self.result.keyframe_counts.values())
        if total_kf > 0:
            stats.append(("ğŸ“Š æ€»å…³é”®å¸§", total_kf))
        
        for label, count in stats:
            line = f"â•‘  {label}: {count}"
            print(line.ljust(51) + "â•‘")
        
        print("â•š" + "â•" * 50 + "â•")
        
        if self.result.check_failed:
            print()
            print("  âš  æ£€æŸ¥æœªé€šè¿‡çš„æ•°æ®:")
            for name in self.result.check_failed:
                print(f"    â€¢ {name}")
        
        if self.result.errors:
            print()
            print("  âŒ å¤±è´¥è¯¦æƒ…:")
            for stem, error_list in self.result.errors.items():
                print(f"    â”Œâ”€ {stem}")
                for step, msg in error_list:
                    # æ˜¾ç¤ºå®Œæ•´é”™è¯¯ä¿¡æ¯ï¼Œæ”¯æŒå¤šè¡Œ
                    print(f"    â”‚  [{step}]")
                    for line in msg.split('\n'):
                        if line.strip():
                            print(f"    â”‚    {line}")
                print(f"    â””â”€")
    
    def _track_single_to_feishu(self, tracker: Tracker, stem: str, silent: bool = False):
        """å•ä¸ªæ•°æ®åŒ…å®Œæˆåç«‹å³åŒæ­¥é£ä¹¦"""
        try:
            kf = self.result.keyframe_counts.get(stem, 0)
            status = "å·²å®Œæˆ" if stem in self.result.check_passed else "æ£€æŸ¥ä¸é€šè¿‡"
            uploaded = stem in self.result.moved_to_final or stem in self.result.skipped_server_exists
            
            record = TrackingRecord(
                name=stem,
                keyframe_count=kf,
                annotation_status=status,
                uploaded=uploaded,
            )
            
            result = tracker.track([record], str(self.json_dir), "configs/pipeline.yaml")
            if result and not silent:
                logger.info(f"é£ä¹¦å·²åŒæ­¥: {stem}")
        except Exception as e:
            logger.warning(f"é£ä¹¦åŒæ­¥å¤±è´¥ {stem}: {e}")
    
    def _track_to_feishu(self):
        """å°†å¤„ç†ç»“æœåŒæ­¥åˆ°é£ä¹¦è¡¨æ ¼ï¼ˆåŒ…æ‹¬è·³è¿‡çš„æ–‡ä»¶ï¼‰"""
        try:
            tracker = Tracker()
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦åŒæ­¥çš„è®°å½•
            records = []
            all_names = set()
            all_names.update(self.result.skipped_server_exists)
            all_names.update(self.result.check_passed)
            all_names.update(self.result.check_failed)
            all_names.update(self.result.moved_to_final)
            
            for name in sorted(all_names):
                status = "å·²å®Œæˆ" if name in self.result.check_passed else "æ£€æŸ¥ä¸é€šè¿‡"
                uploaded = name in self.result.moved_to_final or name in self.result.skipped_server_exists
                records.append(TrackingRecord(
                    name=name,
                    keyframe_count=self.result.keyframe_counts.get(name, 0),
                    annotation_status=status,
                    uploaded=uploaded,
                ))
            
            if not records:
                return
            
            print()
            print(f"  ğŸ“¤ åŒæ­¥åˆ°é£ä¹¦: {len(records)} æ¡è®°å½•...")
            
            result = tracker.track(records, str(self.json_dir), "configs/pipeline.yaml")
            
            if result:
                created = result.get('created', 0)
                updated = result.get('updated', 0)
                if isinstance(created, list):
                    created = len(created)
                if isinstance(updated, list):
                    updated = len(updated)
                print(f"  âœ… é£ä¹¦åŒæ­¥å®Œæˆ: æ–°å¢ {created}, æ›´æ–° {updated}")
        except Exception as e:
            logger.warning(f"é£ä¹¦è¿½è¸ªå¤±è´¥: {e}")
            print(f"  âš  é£ä¹¦åŒæ­¥å¤±è´¥: {e}")
